---
title: "Predicting Sales for E-commerce Project Report"

output: pdf_document
---

```{r package load, echo=FALSE, warning=FALSE, message=FALSE}
if(!require("pacman")) install.packages("pacman")
pacman::p_load(gplots, ggplot2, GGally, data.table,leaps,forecast,
               tidyverse,ggcorrplot,corrplot,MASS,reshape, reshape2, 
               caret, glmnet, randomForest, rpart, rpart.plot, car)
```

```{r data load, echo=FALSE}
df = read.csv("summer-products-with-rating-and-performance_2020-08.csv")
```

## Executive Summary

Online shopping is rapidly becoming a large part of the retail industry and with it has come a massive increase in the amount of usable data for analysis. Companies not only record data on the sales figures of their products, but also data on customers, product data, and many even have data on product ratings generated by actual customers. In this paper, we study the data of such an e-commerce clothing company - **WISH** and consists of their 2020 Summer Edition clothing data. We have used this data to analyze why certain products are doing better in terms of sales, and predict the sales by accounting for various other explanatory variables. For this analysis we used several classification techniques including logistic regression, LDA, and Random Forest.

We ultimately found that predicting how well a given product will sell is difficult to model, and thus suggest further research into this topic with larger data sets and more sophisticated nonlinear models. We also found that the correlation between rating and units sold was statistically significant and that it was an important variable in our Random Forest models. Interesting enough though merchant rating actually proved to be a more important variable in all of our models. Neither of these proved to be particularly powerful predictors though as can be seen by our poor model performances.

\newpage

## A. Introduction

The activity of buying products through online servies over the internet is increasing day by day and in turn driving the technological advances in retail industry. These e-commerce businesses have now started to collect data regarding activites on their websites using certain analytical tools which also enables them to record their sales, customer movements on their website, and all the other vital information which can be scraped from their website.

As the competition is getting substantial within the e-commerce industry it is very important for these businesses to have an edge over their competitors. Proper data recording and data analysis could prove to be just the edge businesses need, helping them to better understand their customers and ensure the efficient and smooth running of their various business activities. 

The data that is produced and collected by these e-commerce businesses are beneficial in several ways -

(i) Foresee the Trends - Businesses can forecast the sales by pulling the data from multiple sources and using correct analysis and/or can also track certain type of data related to advertising or customer behaviour.

(ii) Determine competitive Retail Price - This process can be done by collecting and comparing prices of various products from all over the internet.

(iii) Personalize Customer Experience - Advanced personalization is one of the biggest trends in e-commerce industry over the last few years. Modern browsers are able to memorize the requests of users, their preferences and store this information for personalizing the user experience. E-commerce businesses can do the same and monitor users’ preferences to suggest the most relevant goods.

(iv) Improve Business Service - Having a commendable customer service is a key to thrive in any industry, that is why e-commerce businesses have to be client-oriented. Thus, it is important for businesses to monitor customer reviews and feedback in terms of improving their services. A proper analysis of data related to customer feedback can help in identifying probelms related to service and will help in eliminating it before they lead to a considerable expense.

(v) Boost Sales - After all to grow any business, taking care of sales is a must. Periodical data maintenance and a thorough analysis of the data will help in making data-driven decisions which in turn will help in optimizing the business effectively by observing the holistic picture.

These are some of the ways that maintaining and utlising the data can be beneficial to e-commerce business for monitoring and exapanding the business processes. 


In this project though, we'll seek to help e-commerce business make better data-driven decisions by answering these questions. Can we predict how well a product is likely to sell? And is there any correlation between the “Rating” variable and the “Units sold” variable?



## B. Data Description

This data comes from an e-commerce clothing company - **WISH**. The data contains 1573 records containing product listings as well as products ratings and sales performance. The products listed in this dataset are 2020 summer edition clothing products that are sold on the website. In total there are 43 variables with data types as follows: 24 integer, 16 string, 3 url. The data types are unaltered and are in the exact way they were scraped from the site, they will be changed in the preprocessing part for analysis purpose.

The data was scraped with french localisation thus the product have french writings in the title column, the title_orig on the other hand contains the original title.

There is a difference between *price* column and *retail_price* column. *Price* contains the value for which the product is availabe on site after discount if there is any on that product whereas the *retail_price* contains the retail value of the product.  

The dataset also contains the currnecy used by the buyer to purchase the product, under the column name *currency_buyer*.

The e-commerce website also had the option for the products to choose whether they want to use ad booster for the product and this data is also contained in this dataset, under the column *uses_ad_boost*.

Along with the ratings of the products, this dataset also contains the rating count for all the products available. This count is then further divided by recording the count of rating stars i.e., how many ratings are 5 star, 4 star, 3 star, 2 star, 1 star, respectively.

This dataset also contains the count of badges a product has. A product may have a badge that denotes whether it is a local product, a badge that denotes the quality of the product and another badge that denotes whether the product provides express shipping. All these badges are accounted for and are included in this dataset.

The website actually does not have the concept of categories. Rather it uses tags(keywords) that sellers can set, and this is also recorded and included in this dataset.

This dataset also contains the characteristics such as product color and product size variation. Along with this, the dataset also includes the inventory of all products and a picture of the product.

This dataset also contains information regarding the shipping options available for the consumers along with shipping price and shipping type(whether it is express shipping) and also the number of countries where the product have shipped already.

There is information regarding the merchant who is selling the products in this dataset, the information includes *merchant_name*, *merchant_id*, *merchant_rating* and profile picture of the merchant.

This was a brief description of the dataset that we will be working on. Now we will move on to preprocessing the data and exploratory data analysis.


## C. Data Preprocessing

Data preprocessing is an essential task for any data science project. 

Before we start working on the data, the data needs to be cleaned and prepared for appropriate analysis that are to be conducted upon the dataset. First of all we should get rid of the variables which do not make sense from either a statistical or predictive modeling standpoint. So, we dropped 18 out of 43 variables. Following are the reasons for dropping the variables -

(i) Some did not make sense from analysis point of view.
(ii) There were a lot of missing values in certain variables 
(iii) There a few varaibles containing url values which were not helpful in any analysis

```{r data cleaning, echo=FALSE}
df$title = NULL
df$title_orig = NULL
df$currency_buyer = NULL
df$shipping_option_name = NULL
df$tags = NULL
df$has_urgency_banner = NULL
df$urgency_text=NULL
df$merchant_title=NULL
df$merchant_has_profile_picture=NULL
df$merchant_info_subtitle=NULL
df$merchant_id=NULL
df$merchant_profile_picture=NULL
df$product_url=NULL
df$product_picture=NULL
df$theme=NULL
df$crawl_month=NULL
df$product_id=NULL
df$merchant_name=NULL
```


Now, let’s check out the summary of the data:

```{r data summary, echo=FALSE}
summary(df)
```

Summary of the data highlights the descriptive statistics of the data and it also shows that if any variable has missing values or not. As we can see there are missing variables in the following 
variables:

•	rating_five_count

•	rating_four_count

•	rating_three_count 

•	rating_two_count

•	rating_one_count

There are three ways we can handle the missing values:

•	Deleting the observation

•	Deleting the variable

•	Imputation with mean/median/mode

Since there are 45 missing values observations in each of the variable it is not recommended to delete these observations as these variables are important from predictive modeling standpoint. Upon further examination, we found this missing data came from records with 0 ratings overall. Thus we imputed the missing values with 0 as a product with 0 ratings overall would obviously also have no 1 star or 5 star ratings either.

```{r imputing data, echo=FALSE}
df$rating_five_count[is.na(df$rating_five_count)] <- 0
df$rating_four_count[is.na(df$rating_four_count)] <- 0
df$rating_three_count[is.na(df$rating_three_count)] <- 0
df$rating_two_count[is.na(df$rating_two_count)] <- 0
df$rating_one_count[is.na(df$rating_one_count)] <- 0
```

Now, let’s take a look at structure of the data:

```{r data structure, echo=FALSE}
str(df)
```

Following are the variables that needs their datatypes change from “int” to “factor”:

•	*uses_ad_boosts* – Since this variable is a dichotomous type of variable (Y/N) the datatype cannot be “int”

•	*shipping_is_express* – Since this variable is also a dichotomous variable (Y/N) the data type cannot be “int”

•	*shipping_option_price* – Since this variable is a categorical variable the data type cannot be “int”

We will use as.factor function to convert the variables.

```{r data type conversion, echo=FALSE}
df$uses_ad_boosts = as.factor(df$uses_ad_boosts)
df$shipping_is_express = as.factor(df$shipping_is_express)
df$shipping_option_price = as.factor(df$shipping_option_price)
```

Now that we have a clean dataset, let's jump on to Exploratory Data Analysis to analyze the data more exhautively and to summarise main characteristics of the data with visual methods.

## D. Exploratory Data Analysis

Exploratory Data Analysis is taking a bird’s eye view of the data before any implementation of statistical techniques. EDA is a vital step in any predictive modeling process. Main goal of EDA is to develop an understanding of the data.

EDA is basically an approach to analyze and explore the data visually and statistically, summarize the main characteristics, and identify potential information the data offers. The focus of EDA is not a set of techniques but the attitude or philosophy about how data analysis should be carried out. 

Goals of EDA -

•	Assist in looking at large data by hiding certain aspects of the data while making other aspects more clear.

•	Identifying systematic relations between variables when there is no prior expectation as to the nature of those relations.

•	Maximise insight into a data set.

Since we are interested in predicted sales, let's understand the variable *units_sold* a little better.
```{r units_sold summary, echo=FALSE}
mean(df$units_sold)
hist(df$units_sold, main = "Sales Distribution", col = "blue")
```

The mean amount of sales is **4339** units

The histogram and the boxplot tells us that *units_sold* variable is highly positive skewed. Thus, we need to apply log transformation to get a more normal distribution.

Following is the distribution of the *units_sold* variable after log transformation:

```{r log_units_sold visualisation, echo=FALSE }
df <- transform(df, log_units_sold = log(df$units_sold))
hist(df$log_units_sold, main = "Log of Sales Distribution", col="grey")
```

This looks more normally skewed, but there appears to be some irregularities in the data.

Further study of this variable tells us that the values that are in this variable are discrete and not continuous, moreover there are only 15 different values that are stored in this variable. 

```{r units_sold conversion, echo=FALSE}
list = as.factor(df$units_sold)
summary(list)
```

So for better predictive modeling we will convert this variable into two new response variables, the first one being big_seller which is 1 if units sold is greater than or equal to 10,000 and 0 otherwise; and the second one being sales which splits units sold into 5 different classes as described below:

1 if the quantity of units sold is less than  or equal to 500

2 if the quantity of units sold is less than or equal to 1000 but greater than 500

3 if the quantity of units sold is less than  orequal to 5000 but greater than 1000
 
4 if the quantity of units sold is less than or equal to 10000 but greater than 5000
 
5 if the quantity of units sold is greater than or equal to 10000 but greater than 10000

```{r units_sold one hot encoding, echo=FALSE}
df$big_seller = ifelse(df$units_sold >= 10000, 1,0)
df$big_seller = as.factor(df$big_seller)

df$sales= ifelse(df$units_sold<=500,1,
          ifelse(df$units_sold<=1000,2,
          ifelse(df$units_sold<=5000,3,
          ifelse(df$units_sold<=10000,4,5
           ))))
df$sales = as.factor(df$sales)
df$sales = ordered(df$sales, levels = c(1,2,3,4,5))
```

Next, we look into the variable *price*.

```{r price summary, echo = FALSE}
mean(df$price)
hist(df$price, main = "Price Distribution", col = "blue")
```

The average price of a product in this data is 8.33 euros (since the variable *currency_buyer* suggests that all the transactions were made in euros)

The histogram tells us that *price* is also positively skewed, thus we must apply log transformation to get a more normally distributed data.

Following is the distribution of the *price* variable after log transformation:

```{r log_price visualisation, echo=FALSE}
df <- transform(df, log_price = log(df$price))
hist(df$log_price, main = "Log of Price Distribution", col = "grey")
```

Now we have a normally distributed *price* variable.

Next, we found that the variable *retail_price* was also positivley skewed and after applying log transformation we got a more normally distributed data.

```{r retail_price transformation, echo = FALSE}
hist(df$retail_price, main = "Retail Price Distribution", col = "blue")
df <- transform(df, log_retail = log(df$retail_price))
hist(df$log_retail, main = "Log of Retail Price Distribution", col="grey")
```

Noe if we look at the the variables *product_color* and *product_variation_size_id*, these variables have multiple values which actually have the same meaning, so we will take these variables and create new variables that have these values grouped by like kinds. For example, values stored in *product_color* include - "light grey", "grey", "gray", "Grey", so all these values can be stored under one group i.e "Grey". Same goes for *product_varaition_size_id*, the values stored include - "Size -XXS","SIZE-XXS","XXS", so all these values can be stored under one group i.e "XXS". 

```{r product_color and product_variation_size_id one hot encoding, echo=FALSE}
#working with Product_color

df$Pcolor5= ifelse(df$product_color %in% c("armygreen","khaki","camouflage","mintgreen","lightgreen","lightkhaki","Army green","army green","darkgreen", "Green","green","fluorescentgreen","applegreen","navy"),"green",
            ifelse(df$product_color %in% c( "Black","black & white","black & blue","coolblack","black & green","black & yellow","black"),"black",
            ifelse(df$product_color %in% c("navyblue","lightblue","skyblue","Blue","darkblue","navy blue","navyblue & white","lakeblue","blue"),"blue",
            ifelse(df$product_color %in% c("Yellow","lightyellow","star","yellow"),"yellow",
            ifelse(df$product_color %in% c("offwhite","White","whitefloral","white & black","white & green","white"),"white",
            ifelse(df$product_color %in% c("rosered","rose","Pink","Rose","pink & grey","floral","lightpink","pink & white","pink & black","pink & blue","dustypink","pink"),"pink",
            ifelse(df$product_color %in% c("Red","rouge","lightred","coralred","watermelonred","Rouge","red"),"red",
            ifelse(df$product_color %in% c("Orange","orange-red","apricot","orange"),"orange",
            ifelse(df$product_color %in% c("coffee","brown"),"brown",
            ifelse(df$product_color %in% c("lightgrey","gray","Grey","grey"),"Grey",
            ifelse(df$product_color %in% c ("beige"),"beige",
            ifelse(df$product_color %in% c ("purple"),"purple","other"))))))))))))

df$Pcolor5 = as.factor(df$Pcolor5)


# working with size

df$size <- ifelse(df$product_variation_size_id %in% c("28","29", "Size -XXS","SIZE-XXS","XXS", "XXXS"),"XXS",
           ifelse(df$product_variation_size_id %in% c( "30","31","XS.","Size-XS", "SIZE XS","XS"),"XS",
           ifelse(df$product_variation_size_id %in% c("32","33","S.","Suit-S","Size S","size S","Size--S","Size-S","S Pink","s","S"),"S",
           ifelse(df$product_variation_size_id %in% c("34","M.","Size M","M"),"M",
           ifelse(df$product_variation_size_id %in% c("35","L.","SizeL","L"),"L",
           ifelse(df$product_variation_size_id %in% c("36","X   L","XL"),"XL" ,
           ifelse(df$product_variation_size_id %in% c("37","2XL","XXL", "XXXL"),"XXL", "other")))))))

df$size = as.factor(df$size)

#deleting variables that are modiefied
df$product_color<-NULL
df$product_variation_size_id<-NULL
```




We also found that varaible *rating* and *merchant_rating* were negatively skewed and had some outliers, but after thoroughly checking and identifying those outliers we decided to keep the observations as it is because the reason behind these outliers were that some of the products and merchants respectively had poor ratings.

After studying the collection and storing methodology of the variable *rating*, we get to know that to promote the sales of certain products, the automatic rating for that variable will be 5.0 when the *rating_count* is 0. So to fix this biasness in the data we will interchange the rating where *rating_count* is 0 and *rating* is 5.0 with average rating of all the products.

```{r rating imputation, echo=FALSE}
mean_rating = mean(df[df$rating_count != 0,]$rating)

for (x in 1: nrow(df)){
  if (df$rating_count[x] == 0){
    df$rating[x] = mean_rating
  }
}
```

Next we look into the correlation between the variables that are deemed to be important for our analysis purpose using a correlation plot:

```{r correlation plot, echo=FALSE, message= FALSE}
ggpairs(df[, c(5,16,19,20,23,24,27,28)])
```

Now that we have a clean dataset and as well as the understanding of the data, we can use this dataset now to perform various multivariate statistical and modelling techniques to suffice our need of predictive analysis.

## E. Empirical Analysis

```{r training and test data set, echo=FALSE}
set.seed(42)
train.index <- sample(nrow(df),round(nrow(df)*0.8))
train.df <- df[train.index,]
test.df <- df[-train.index,]

```


```{r logit regression, echo=FALSE}
sales.logit1 = glm(big_seller ~ log_price + log_retail + rating + 
                     merchant_rating + uses_ad_boosts + badge_local_product +
                     badge_product_quality +  badge_fast_shipping +
                     shipping_option_price + shipping_is_express +
                     countries_shipped_to + origin_country + Pcolor5 + size,
                   data = train.df, family = "binomial") 
```

```{r logit regression summary, echo = FALSE, eval= FALSE}
summary(sales.logit1)

```


We started our analysis by running a logistic regression on our big_seller variable and then checking it's accuracy on the test data set to see if this is a good model for predicting which items will be high sellers. The classification accuracy we obtained from this model was around 80% which at first seemed promising until it is noted that the NIR is around 80% as well. Ultimately this model isn't doing much better than simply classifying all items as low sellers. 


```{r logit confusion matrix, echo=FALSE}
sales.logit1.pred <- predict(sales.logit1, test.df, type = "response")
sales.logit.tab = table(ifelse(sales.logit1.pred>0.5,1,0), test.df$big_seller)
confusionMatrix(sales.logit.tab)
```




Next we performed variable selection on our logistic model using the stepAIC funciton to see if this would help model performance. This ended up dropping the variables log_price, two of the badge variables, all the shipping price options, and all of the color varialbes as well. Improvements to classification accuracy were minimal though. 
As we predicted though the estimate of the coefficient for ratings is indeed statistically significant (p-value = 0.0011) with an increase in the ratings score by 1 star increasing the odds ratio of being a high seller on average by about 84%. Interesting to note though is that merchant rating has the most statistically significant effects on the likelihood of being a big seller as well as one of the largest coefficients. Neither of these appear to be particularly powerful predictiors though given our model performance.


```{r logit step AIC, echo=FALSE}
sales.logit2 = stepAIC(sales.logit1, trace = 0)

summary(sales.logit2)
```

```{r logit step confusion matrix, echo=FALSE}
sales.logit2.pred <- predict(sales.logit2, test.df, type = "response")
sales.logit2.tab = table(ifelse(sales.logit2.pred>0.5,1,0), test.df$big_seller)
confusionMatrix(sales.logit2.tab)

```



We then ran an LDA analysis on our data to see if it might perform better at predicting which items will sell the best. We first ran an LDA analysis with big_seller being our response variable, though as can be seen LDA methods seem to struggle to seperate out the two groups in big_seller and thus once again the accuracy of classification on the test data set is around the NIR.


```{r LDA big_seller, echo=FALSE}
norm.values  = preProcess(train.df[ , -c(25,26)], method = c("center", "scale"))

sales.train.norm <- predict(norm.values, train.df)
sales.test.norm <- predict(norm.values, test.df)


sales.lda1 <- lda(big_seller ~ log_price + log_retail + rating + 
                     merchant_rating + uses_ad_boosts + badge_local_product +
                     badge_product_quality +  badge_fast_shipping +
                     shipping_option_price + shipping_is_express +
                     countries_shipped_to + origin_country + Pcolor5 + size,
                     data = sales.train.norm)
```


```{r LDA big_seller summary, echo = FALSE, eval = FALSE}
sales.lda1
```

```{r LDA big_seller confusion matrix , echo= FALSE}
sales.lda1.pred = predict(sales.lda1, sales.test.norm)
confusionMatrix(sales.lda1.pred$class, sales.test.norm$big_seller)
```



We then tried running an LDA analysis using sales as our response variable to see if seperating our response variable into multiple classes might improve the model any. LDA still seems to have trouble seperating out the classes from each other though, and thus the classification accuracy of our model is again around the NIR.



```{r LDA sales, echo=FALSE}
sales.lda2 <- lda(sales ~ log_price + log_retail + rating + 
                     merchant_rating + uses_ad_boosts + badge_local_product +
                     badge_product_quality +  badge_fast_shipping +
                     shipping_option_price + shipping_is_express +
                     countries_shipped_to + origin_country + Pcolor5 + size,
                     data = sales.train.norm)
```

```{r LDA sales summary, echo = FALSE , eval=FALSE}
sales.lda2
```

```{r LDA sales confusion matrix , echo= FALSE}
sales.lda2.pred = predict(sales.lda2, sales.test.norm)
confusionMatrix(sales.lda2.pred$class,sales.test.norm$sales )
```




Lastly seeing that linear models were having a hard time capturing the relationship between these variables, we decided to try a nonlinear model to see if this would give us a better predictor of how well an item would sell. To this end we ran a Random Forest analysis on our two response variables.

```{r RandomForest big_seller, echo=FALSE}
set.seed(42)
sales.rf1 <- randomForest(big_seller ~ log_price + log_retail + rating + 
                     merchant_rating + uses_ad_boosts + badge_local_product +
                     badge_product_quality +  badge_fast_shipping +
                     shipping_option_price + shipping_is_express +
                     countries_shipped_to + origin_country + Pcolor5 + size,
                   data = df, subset = train.index, mtry = 4, importance = TRUE)
```

```{r RandomForest big_seller variable importance, echo=FALSE, eval = FALSE}
#importance(sales.rf1)
varImpPlot(sales.rf1)
```


This model ultimately gave us the best accuracy rate for predicting big_seller, however it is not statistically different than the NIR, thus this model ultimately isn't doing a good job at predicting high sellers either. 


```{r RandomForest big_seller confusion matrix, echo=FALSE}
sales.rf1.pred <- predict(sales.rf1, newdata = df[-train.index,], type = "class")
confusionMatrix(sales.rf1.pred,test.df$big_seller)
```



Random Forest did much better though when we used it on the sales variable. Although accuracy is only around 0.52 this is statistically quite different from the NIR. Thus although this still isn't a great predictor of sales it is at least picking up on some relationship between our explanatory variables and our response variable. It's interesting to note then that merchant rating, product rating, size, and color were the most important variables in this model.

```{r RandomForest sales, echo=FALSE}
set.seed(42)
sales.rf2 <- randomForest(sales ~ log_price + log_retail + rating + 
                     merchant_rating + uses_ad_boosts + badge_local_product +
                     badge_product_quality +  badge_fast_shipping +
                     shipping_option_price + shipping_is_express +
                     countries_shipped_to + origin_country + Pcolor5 + size,
                   data = df, subset = train.index, mtry = 4, importance = TRUE)

#importance(sales.rf2)
varImpPlot(sales.rf2)
```

```{r RandomForest sales confusion matrix, echo=FALSE}
sales.rf2.pred <- predict(sales.rf2, newdata = df[-train.index,], type = "class")
confusionMatrix(sales.rf2.pred, test.df$sales)
```






## F. Conclusions

Ultimately we found that predicting how well a product will sell is quite difficult. Either other explanatory variables outside of our model are more important in predicting units sold or the relationship between these explanatory variables and our response variables are highly nonlinear. Thus we recommend further investigation of this subject using more complicated nonlinear methods of analysis. Despite our difficulties in predicting how well a given product would sell, we did find a few interesting findings. The most interesting being that rating was indeed a statistically significant variable in the logistic regression and an important variable in Random Forest. Even more interesting though is that merchant_rating was even more statistically significant and was a more important variable in the Random forest model. Obviously though neither of these were particularly powerful predictors given our classification accuracies.




